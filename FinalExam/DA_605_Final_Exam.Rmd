---
title: "DA 605 - Final Exam"
author: "Dan Fanelli"
date: "May 14, 2016"
output: pdf_document
---


```{r echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
rm(list = ls(all = TRUE)) # make sure previous work is clear
library(Matrix)
library(pracma)
library(sgd)
library(knitr)
library(ggplot2)
```

---

# 1. Review of Essential Concepts - 15 points

---

### 1) What is the rank of the following matrix?

```{r echo=FALSE, warning=FALSE, error=FALSE}
values_q1 <- c(1,2,6,-1,1,-1,3,5,-2,-5,-9,4)
mtrx_q1 <- matrix(values_q1, nrow=3)
mtrx_q1
```

The matrix rank is: __3__

```{r eval=FALSE, echo=FALSE}
rankMatrix(mtrx_q1)
```

---

### 2) What is the reduced row-echelon form of the above matrix?

The rref of the above matrix is:

```{r echo=FALSE, warning=FALSE, error=FALSE}
rref_q2 <- rref(mtrx_q1)
rref_q2
#rankMatrix(rref_q2)
```

---

### 3) Define orthonormal basis vectors. Please write down at least one orthonormal basis for the 4-dimensional vector space R4.

Orthonormal basis vectors are unit vectors that are orthogonal to each other. (their dot product == 0)

```{r echo=TRUE, warning=FALSE, error=FALSE}
q3_v1 <- c(1/3,2/3,2/3,0)
q3_v2 <- c(2/3,1/3,-2/3,0)
sum(q3_v1 * q3_v2)
```

---

### 4) Given the following matrix, what is its characteristic polynomial?

x^3 - 13x^2 + 46x - 26

```{r echo=FALSE, warning=FALSE, error=FALSE}
values_q4 <- c(6,0,-1,1,7,3,1,-1,0)
mtrx_q4 <- matrix(values_q4, nrow=3)
print(mtrx_q4)
charpoly(mtrx_q4)
```

---

### 5) What are its eigenvectors and eigenvalues? Please note that it is possible to get complex vectors as eigenvectors.

Eigenvalues:

```{r echo=FALSE, warning=FALSE, error=FALSE}
eigen(mtrx_q4)$values
```

Eigenvector #1:

```{r echo=FALSE, warning=FALSE, error=FALSE}
eigen(mtrx_q4)$vectors[,1]
```

Eigenvector #2:

```{r echo=FALSE, warning=FALSE, error=FALSE}
eigen(mtrx_q4)$vectors[,2]
```

Eigenvector #3:

```{r echo=FALSE, warning=FALSE, error=FALSE}
eigen(mtrx_q4)$vectors[,3]
```

---

### 6) Given a column stochastic matrix (all col values add to 1 and no negatives) of links between URLs, what can you say about the PageRank of this set of URLs? How is it related to its eigendecomposition?

PageRank is be done by estimating the Principal Eigenvector (the eigenvector corresponding to the largest eigenvalue) of the column stochastic matrix generated by the Eigen Decomposition.

---

### 7) Assuming that we are repeatedly sampling sets of numbers (each set is of size n) from an unknown probability density function. What can we say about the mean value of each set?

The mean value of the each set will be normally distributed and have a mean that is approximately the mean of the parent set.

---

### 8) What is the derivative of (e^x)*((sin(x))^2) ?

Product Rule: 

= (e^x)' * ((sin(x))^2) + (e^x) * ((sin(x))^2)'

= ((e^x) dx * ((sin(x))^2) + (e^x) * 2 * sin(x) * cos(x)) dx

---

### 9) What is the derivative of log(x^3 + sin(x))?

= ((1/log(x^3 + sin(x)))*(d/dx x^3 + sin(x))) dx

= ((1/log(x^3 + sin(x)))*(3x^2 + cos(x))) dx

---

### 10) What is integral(e^x cos(x) + sin(x)dx) ? Don't forget the constant of integration.

= integral(e^x cos(x) + sin(x) dx)

= integral(sin(x)dx) + integral(e^x cos(x) dx)

= -cos(x) + integral(e^x cos(x))

(call the above the "___left___" and the "___right___", now just work on the right:)

---

integral (e^x cos(x)) dx

___(right:) integration by parts I, let:___

u = e^x, so du = e^x dx

dv = cos(x) dx, so v = sin(x)

so integral(e^x cos(x)) dx = e^x * sin(x) - integral(e^x sin(x))

___(right:) integration by parts II, let:___

u = e^x, so du = e^x dx

dv = sin(x) dx, so v = - cos(x)

so integral(e^x sin(x) dx) = - e^x cos(x) - integral(- e^x cos(x) dx)

so integral(e^x sin(x) dx) = - e^x cos(x) + integral(e^x cos(x) dx)

___(right:) substitution:___

So now let's fully  write out the "right":

integral (e^x cos(x)) dx = e^x * sin(x) - [integral(e^x * sin(x) dx)]

integral (e^x cos(x)) dx = e^x * sin(x) - [- e^x cos(x) + integral(e^x cos(x) dx)]

integral (e^x cos(x)) dx = e^x * sin(x) + e^x cos(x) - integral(e^x cos(x) dx)

___(right:) integral (e^x cos(x)) is on both sides of equation, so solve:___

2 * integral (e^x cos(x)) = e^x * sin(x) + e^x cos(x) 

integral (e^x cos(x)) = (1/2)(e^x * sin(x) + e^x cos(x)) + C

___Final answer: combine the left and right:___

integral(e^x cos(x) + sin(x)dx) = -cos(x) + (e^x * sin(x) + e^x cos(x))/2 + C

---

# 2. Mini-coding assignments - 15 points

---

### 2.1. Sampling from function.

Assume that you have a function that generates integers between 0 and 50 with the following probability distribution: 

P(x == k) = (50 choose k) * (p^k) * (50^(q-k))

where p = 0.2 and q = 1 - p = 0.8 and x is in [0,50]. 

This is also known as a Binomial Distribution. 

* Write a function to sample from this distribution. 
* After that, generate 1000 samples from this distribution and plot the histogram of the sample. 

(Please note that the Binomial distribution is a discrete distribution and takes values only at integer values of x between x in [0,50]. Sampling from a discrete distribution with finite values is very simple but it is not the same as sampling from a continuous distribution.

```{r warning=FALSE, error=FALSE}
prob_of_chooseing <- function(number_k){
  p <- 0.2
  q <- 1-p
  fifty_choose_k <- factorial(50)/(factorial(number_k)*factorial(50-number_k))
  p_to_the_k <- p^number_k
  q_to_the_50_minus_k <- q^(50-number_k)
  return (fifty_choose_k * p_to_the_k * q_to_the_50_minus_k)
}

#--------------------------------------------------------
# not a sample, just the probability distribution plotted:
#--------------------------------------------------------
zero_to_50 <- c(0:50)
probs_of_each <- prob_of_chooseing(zero_to_50)
# confirm it all sums to 1
sum(probs_of_each)
plot(zero_to_50, probs_of_each, main = "Plot of PROBABILITIES of Choosing Number.")

#--------------------------------------------------------
# now the sample:
#--------------------------------------------------------
the_sample <- sample(zero_to_50, size = 1000, replace = TRUE, prob=probs_of_each)
head(the_sample, n=40)
hist(the_sample, main = "Distribution of the Sample based on above Probability Distribution")
```

---

### 2.2. Principal Components Analysis.

First take a look at the data:

```{r warning=FALSE, error=FALSE}
auto_df <- as.data.frame(read.table("auto-mpg.data"))
colnames(auto_df) <- c("disp","hp","wt","acc","mpg")
pairs(auto_df)
# Seems like highly correlated, good candidate for PCA.
```

* For the auto data set attached with the final exam, please perform a Principal Components Analysis by performing an SVD on the 4 independent variables (with mpg as the dependent variable) and select the top 2 directions. 

```{r warning=FALSE, error=FALSE}
# nv param tells it to only give back 2 directions
auto_df_svd <- svd(auto_df[,1:4], nv=2)
```

* Please scatter plot the data set after it has been projected to these two dimensions. 
* Your code should print out the two orthogonal vectors and also perform the scatter plot of the data after it has been projected to these two dimensions.


```{r warning=FALSE, error=FALSE}
# columns of U from the SVD correspond to the principal components
plot(auto_df_svd$u[, 1], auto_df_svd$u[, 2], main = "SVD", xlab = "U1", ylab = "U2")

# first eigen vector:
auto_df_svd$u[,1]
# second eigen vector:
auto_df_svd$u[,2]
# orthogal vectors have dot product of zero
round(auto_df_svd$u[,1] %*% auto_df_svd$u[,2])
```

---

### 2.3 Sampling in Bootstrapping.

As we discussed in class, in bootstrapping we start with n data points and repeatedly sample many times with replacement. Each time, we generate a candidate data set of size n from the original data set. All parameter estimations are performed on these candidate data sets. It can be easily shown that any particular data set generated by sampling n points from an original set of size n covers roughly 63.2% of the original data set. Using probability theory and limits, please prove that this is true. After that, write a program to perform this sampling and show that the empirical observation also agrees this.

___Proof (with probability theory and limits)___

* For collection of integers from 1 to n, let S be collection of those integers drawn randomly and with replacement 
* One any one particular sample from S, the probability of drawing the number s1 is 1/n.
* Therefore, the probability of NOT drawing the number s1 on that sample is 1-(1/n)
* Likewise, the probability of NOT drawing the number s2 on that sample is also 1-(1/n)
* Therefore, the probability of NOT drawing the number s1 OR s2 on that sample is (1-(1/n))^2
* Thus for a subset of size n of the original set S, the probability of not drawing ANY of the integers in that size-n set is (1-(1/n))^n
* Now take the limit as n -> infinity of (1-(1/n))^n, and you get 1-(1/e), or about 0.632

___Proof (empirical)___

```{r warning=FALSE, error=FALSE}
boostrap_sample <- function(n_num_data_points){
  the_sample <- sample.int(n_num_data_points, size = n_num_data_points, replace = TRUE)
  the_sample <- sort(unique(the_sample))
  pct_found <- length(the_sample)/n_num_data_points
  return (pct_found)
}

boostrap_sample(100)
boostrap_sample(1000)
boostrap_sample(10000)
```


---

# 3. Mini-project - 20 points

In this mini project, you'll perform a Multivariate Linear Regression analysis using Stochastic Gradient Descent. The data set consists of two predictor variables and one response variable. The predictor variables are living area in square feet and number of bedrooms. The response variable is the price of the house. You have 47 data points in total.

Since both the number of rooms and the living area are in different units, it makes it hard to compare them in relative terms. One way to compensate for this is to standardize the variables. In order to standardize, you estimate the mean and standard deviation of each variable and then compute new versions of these variables. For instance, if you have a variable x, then then standardized version of x is xstd = (x - mew)/sigma where mew and sigma are the mean and standard deviation of x, respectively.

```{r warning=FALSE, error=FALSE}
x_predictors <- read.table("mini-project-data/ex3x.dat", header=FALSE)
colnames(x_predictors) <- c("x1","x2")

y_response <- read.table("mini-project-data/ex3y.dat", header=FALSE)
colnames(y_response) <- c("y")

mtrx <- cbind(y_response, x_predictors$x1, x_predictors$x2)
colnames(mtrx) <- c("y","x1","x2")
head(mtrx)
nrow(mtrx)

mtrx$y_std <- (mtrx$y - mean(mtrx$y))/sd(mtrx$y)

mtrx$x1_std <- (mtrx$x1 - mean(mtrx$x1))/sd(mtrx$x1)
mtrx$x2_std <- (mtrx$x2 - mean(mtrx$x2))/sd(mtrx$x2)

head(mtrx)

# plot the 2 independently to get the idea of them...
plot(mtrx$x1_std, mtrx$y_std)
plot(mtrx$x2_std, mtrx$y_std)

```

As we saw in the gradient descent equations, we introduce a dummy variable x0 = 1 in order to calculate the intercept term of the linear regression. Please standardize the 2 variables, introduce the dummy variable and then write a function to perform gradient descent on this data set. You'll repeat gradient descent for a range of alpha values. Please use alpha = (0:001; 0:01; 0:1; 1:0) as your choices.

```{r warning=FALSE, error=FALSE}
x1_and_x2_mtrx <- as.matrix(cbind(mtrx$x1_std, mtrx$x2_std))
y_mtrx <- as.matrix(mtrx$y_std)

# define the gradient function dJ/dtheata: 1/m * (h(x)-y))*x 
# where h(x) = x*theta in matrix form this is as follows:
grad <- function(x, y, theta, alpha) {
  gradient <- (1/nrow(y))* (t(x) %*% ((x %*% t(theta)) - y))
  return(t(gradient))
}
 
# define gradient descent update algorithm
grad.descent <- function(x, y, maxit,alpha, start_pt){
  num_iterations <- 0
  dummy_var <- c(start_pt, start_pt)
  theta <- matrix(dummy_var, nrow=1) # Initialize
  for (i in 1:maxit) {
    this_theta <- theta - alpha  * grad(x, y, theta, alpha)
    # do this so num_iterations will only get incremented if the value is different:
    x_equal <- (round(this_theta[1],digits=5) != round(theta[1],digits=5))
    y_equal <- (round(this_theta[2],digits=5) != round(theta[2],digits=5))
    if(x_equal && y_equal){
      theta <- this_theta
      num_iterations <- i
    }else{
      break
    }
  }

  return (list("theta" = theta, "num_iterations" = num_iterations))
}

```

For each value of alpha perform about 500 SGD iterations with 5 randomly picked samples in each iteration and compute J(theta) at the end of each iteration. When you perform SGD, you randomly pick a mini-batch (in this case 5 samples), use that mini-batch to compute the gradient, and then take a step to improve the objective function. You repeat this process in each iteration. It is very important to randomly pick samples in each iteration - otherwise SGD will not work. 

Please plot J(theta) versus number of iterations for each of the 4 alpha choices.

```{r warning=FALSE, error=FALSE}
# init some constants:
alpha_starts <- c(0.001, 0.01, 0.1, 1.0)
start_pts <- sample(-1000:1000, 5)
five_0s <- c(0,0,0,0,0)
expected_result_x <- 0.885
expected_result_y <- -0.053


calc_results_df <- data.frame(tmp1=five_0s,tmp2=five_0s,tmp3=five_0s,tmp4=five_0s)
colnames(calc_results_df) <- alpha_starts

ggplot_df <- data.frame(tmp1=five_0s,tmp2=five_0s,tmp3=five_0s,tmp4=five_0s)
colnames(ggplot_df) <- alpha_starts

rownames(calc_results_df) <- start_pts
calc_results_df

numiterations_for_alpha <- c()
start_pts_for_alpha <- c()

ALPHAS <- c()
START_PT_X <- c()
START_PT_Y <- c()
END_PT_X <- c()
END_PT_Y <- c()
ITERATIONS <- c()

# set learning rate
for (alpha in alpha_starts){
  results_for_alpha <- c()
  for(start_pt in start_pts){
    func_result <- grad.descent(x1_and_x2_mtrx, y_mtrx, 500, alpha, start_pt)
    ALPHAS <- c(ALPHAS,alpha)
    END_PT_X <- c(END_PT_X,func_result$theta[1])
    END_PT_Y <- c(END_PT_Y,func_result$theta[2])
    ITERATIONS <- c(ITERATIONS,func_result$num_iterations)
    # these are the same for every col, but helps with calculations later    
    START_PT_X <- c(START_PT_X,start_pt)
    START_PT_Y <- c(START_PT_Y,start_pt)
        
    formatted_result <- paste('(',round(func_result$theta[1,1], digits=5),',',round(func_result$theta[1,2], digits=5),')', sep = "")
    calc_results_df[toString(start_pt),toString(alpha)] <- formatted_result
  }
  
  numiterations_for_alpha <- c(numiterations_for_alpha, func_result$num_iterations)
}

kable(calc_results_df)

FINAL_DF <- data.frame(alpha=ALPHAS, iterations=ITERATIONS, START_X=round(START_PT_X,digits = 3), START_Y=round(START_PT_Y,digits = 3), END_PT_X=round(END_PT_X,digits = 3), END_PT_Y=round(END_PT_Y,digits = 3))

FINAL_DF$DX <- round((FINAL_DF$END_PT_X - expected_result_x),digits = 3)
FINAL_DF$DY <- round((FINAL_DF$END_PT_Y - expected_result_y),digits = 3)

FINAL_DF$SUCCESS <- (abs(FINAL_DF$DX - FINAL_DF$DY) <= 0.001)

# REMINDERS OF WHERE WE WANTED TO GET TO:
expected_result_x
expected_result_y

kable(FINAL_DF)

ggplot(data = FINAL_DF[,c('alpha','iterations','SUCCESS')], aes(x=alpha, y=iterations)) + geom_point(aes(colour=SUCCESS))

```

### Some Poitnts about the Plot:

---

Interesting that sometimes the red dots appeared up at 500 iterations, other times they appeared with values lower.  Values lower show: "Hey we got stuck in a ditch...could keep on iterating up to 500, but not going to go anywhere..."

---

Once you have your final gradient descent solution, compare this with regular linear regression (using the built-in function in R). Please document both solutions in your submission. How does the SGD solution differ from the Linear Regression solution? Are they different? If so, why? If not, why not?

```{r warning=FALSE, error=FALSE}
solved <- solve(t(x1_and_x2_mtrx) %*% x1_and_x2_mtrx) %*% t(x1_and_x2_mtrx) %*% y_mtrx
solved
```

### Answer:

(After rounding the hand calculated results):

They are NOT the same for the smaller alpha "jumps" of 0.001 and 0.01

They ARE the same for the bigger alpha "jumps" of 0.1 and 1

This seems to be because the smaller "jumps" of 0.001 and 0.01 are not large enough to get out of the "valleys", ie - local minimums, while 0.1 and 1 are large enough to get out of those same local minimums.